\documentclass[english]{article}
\usepackage{fullpage}
\usepackage{setspace}
\usepackage{color}
\usepackage{hyperref}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{mathtools}
\input{preamble}

\title{MA4199 Project -- Bias Variance Tradeoff}
\date{}
\author{Ng Wei Le}

%\newcommand{\ntrain}{n_{\rm train}}
%\newcommand{\ntest}{n_{\rm test}}
\newcommand\norm[1]{\left\lVert#1\right\rVert}
%\DeclareMathOperator{\Tr}{Tr}
\onehalfspacing

\begin{document}
\maketitle
%Note: The main reference for sections \ref{sec:DimRed} to \ref{sec:mods} is:  \cite{ShalevBen}.\\
\section{Approximation Theorem} \label{sec:AppThm}
\begin{defn}
	The fill distance for a set of points $X = \{x_1, ..., x_N\} \subseteq \Omega$ for a bounded domain $\Omega$ is defined to be
	\[
	h_{X,\Omega} \coloneqq \sup_{x \in \Omega}\min_{1 \leq j \leq N} \norm{x- x_j}_2
	\].
\end{defn}
The below theorem gives us some justification as to why the minimum norm interpolating function was chosen, though this only works under noiseless conditions:
% Might need to rephrase this part to avoid plag
\begin{thm} \label{thm:approx}
	Fix $h^* \in \RKHSGauss $  .
Let $(x_1,y_1), ..., (x_n,y_n)$ be i.i.d. random variables where $x_i$ drawn randomly from a compact cube $\Omega \subseteq \mathbb{R}^d $,
$y_i = h^*(x_i) \: \forall i$. There exists $A, B > 0$ such that for any interpolating $h \in \mathcal{H}_\infty $ with high probability
\begin{equation*}
\sup_{x \in \Omega} \vert h(x) - h^*(x)\vert < A e^{-B(n/log \, n)^{1/d}} (\| h^* \|_{\mathcal{H}_\infty} + \| h \|_{\mathcal{H}_\infty})
\end{equation*}
\end{thm}
Theorem 11.22 in \cite{ScatteredDataApproximation}:

Let $\Omega$ be a cube in $\RR^d$. Suppose ... There exists a constant $c > 0$ such that the error between a function $f \in N(\Omega)$ and its interpolant $s_{f,X}$ can be bounded by:
\begin{equation*}
\norm{f - s_{f,X}}_{L_\infty(\Omega)} \leq \text{exp}(-c/h_{X,\Omega})|f|_N(\Omega)
\end{equation*}
for all data sites X with sufficiently small $h_{X,\Omega}$.

With $h_{X,\Omega}$ as the fill on the order of $O(n/logn)^{-1/d}$ (using the theorem S1 in Belkin's paper which wasn't proved).
We consider $f(x) := h(x) - h*(x)$. Since $h$ is interpolating, we have $f(x_i) = 0$ for all $x_i$. We then let $s_{f,X}$ be the zero function, since it is an interpolant of $f$. Thus, we have:
$s_{f,X}$ can be bounded by:
\[
\norm{f}_{L_\infty(\Omega)} = \sup_{x \in \Omega} \vert h(x) - h^*(x)\vert <
\text{exp}(-c(n/log n)^{1/d})|f|_N(\Omega)
\]
\[
\leq \text{exp}(-c(n/log n)^{1/d}) (\| h^* \|_{\RKHSGauss} + \| h \|_{\RKHSGauss})
\]
Another form we can have is using proposition 14.1 in \cite{ScatteredDataApproximation}:
\begin{prop}
	Let $\Omega \subseteq \RR^d$ be bounded and measurable. Suppose $X = \{x_1, ... , x_N\} \subseteq \Omega$ is quasi-uniform with respect to  $c_{qu} > 0$. Then there exists constants $c_1, c_2 > 0$ depending only on space dimension $d$, on $\Omega$ and on $c_{qu}$ such that:
	\[c_1N^{-1/d} \leq h_{X,\Omega} \leq c_2N^{-1/d} \].
\end{prop}
With the definition of quasi-uniformness being:
\begin{defn}
	For the separation distance of $X = \{x_1, ... , x_N\}$ being defined as $q_x \coloneqq \half \min_{i\neq j} \norm{x_i - x_j}_2$.
\end{defn}
We can then use the above proposition with $n$ replacing $n/\text{log}n$.

In either case, by choosing a the smallest norm for $h$, we can see that it corresponds to the smallest upperbound for $\vert h(x) - h^*(x)\vert$.

\section{Existing Bounds Provide No Guarantees for Interpolated Kernel Classifiers} \label{sec:BoundsKernel}
Steps are: 
\begin{itemize}
	\item Find lower bound on function norm of t-overfitted classifiers in RKHS corresponding to Gaussian Kernels.
	\item Show loss for available bounds for kernel methods based on function norm (can perhaps use this to explain approximation theorem as well?)
\end{itemize}
Interpolation: 0 regression error. Overfitting: 0 classification error. Interpolation implies overfitting.
\begin{defn}
	We say $h \in H$ t-overfits data, if it achieves zero classification loss (overfits) and $\forall_iy_ih(x_i) > t > 0$.
\end{defn}
The below shows a theorem on how the function norm changes with respect to t-overfitting.
\begin{thm} \label{thm:normBound}
	Let $(\xv_i ,y_i)$ be data sampled from $P$ on $\Omega \times \{-1, 1\}$ for $i = 1,..., n$. Assume that $y$ is not a deterministic function of $x$ on a subset of non-zero measure. Then, with high probability, any $h$ that t-overfits the data, satisfies
	\[\norm{h}_H > Ae^{Bn^{1/d}}\]
	for some constants $A, B > 0$ depending on $t$.
\end{thm}

We define the $\gamma$-shattering and fat-shattering dimension below:
\begin{defn}
	Let F be a set of functions mapping from a domain $X$ to $\RR$. Suppose $S = \{x_1, x_2, ..., x_m\} \subseteq X$.  Suppose also that $\gamma$ is a positive real number. Then $S$ is $\gamma$-shattered by F if there are real
	numbers $r_1, r_2,..., r_m$, such that for each $b \in \{0, 1\}^m$ there is a function $f_b$ in $F$ with
	\[
	f_b(x_i) \geq r_i + \gamma \text{ if } b_i = 1 \text{, and } f_b(x_i) \leq r_i - \gamma \text{ if } b_i = 0 \text{, for } 1 \leq i \leq m.
	\] We say $r = (r_1, r_2,..., r_m)$ witnesses the shattering.
	Suppose that F is a set of functions from a domain $X$ to $\RR$ and that $\gamma > 0$. Then $F$ has $\gamma$-dimension $d$ if $d$ is the maximum	cardinality of a subset $S$ of $X$ that is $\gamma$-shattered by $F$. If no such maximum exists, we say that $F$ has infinite $\gamma$-dimension. The $\gamma$-dimension of $F$ is denoted $fat_F(\gamma)$. This defines a function $fat_F: \RR \rightarrow N \cup \{0,\infty\}$, which we call the fat-shattering dimension of $F$.
\end{defn}
\begin{proof}
Let $B_R = \{ f \in \mathcal{H}, \norm{f}_\mathcal{H} < R \}$ be a ball of radius $R$ in RKHS $\mathcal{H}$. Suppose the data is $\gamma$-overfitted, \cite{LossFATBound} gives us a high probability of a bound of
\[ L(f) < O(\frac{\text{ln}(n)^2}{\sqrt{n}}\sqrt{fat_{B_R}(\gamma/8)}) \] for $L(f)$ the expected classification error. Also, from \cite{ApproximationConcentration} we have \[fat_{B_R}(\gamma) < O((log(R/\gamma))^d)\].
We then have $B_R$ containing no function that $\gamma$ overfits the data unless
\[(log(R/\gamma))^d > O(n) \implies R > c_1 \text{ exp}(c_2(\frac{n}{\text{ln }n})^{1/d})\]
for some positive constants $c_1, c_2$.
\end{proof}
Classical bounds for kernel methods (\cite{UnderstandKernel} ) are in the form:
\[ \lvert \frac{1}{n} \sum_{i}l(f(x_i),y_i) - L(f) \rvert \leq C \frac{\norm{f}^a_\mathcal{H}}{n^b}, \hspace{1em} C,a,b \geq 0 \]
The right side on this will tend to infinity for bigger $\norm{f}_\mathcal{H}$, which is suggested by Theorem \ref{thm:normBound}.

\section{Random Fourier Features} \label{sec:RFFs}
For a feature map $\phi: \RR^d \rightarrow \RR^{d'}$ the kernel trick allows easy computation for positive definite kernel $k$ where $k(x,y) = <\phi(x), \phi(y)>$. We want to find a randomized feature map $z: \RR^d \rightarrow \RR^{\bar{d}}$ such that 
\[ k(x,y) = <\phi(x), \phi(y)> \approx <z^{\Trans}(x), z(y)> \].
As suggested by \cite{RFF_Rahimi}, for a shift-invariant kernel $k$: $k(x, y) = k(x - y)$), we consider the mapping $z(x) = cos(w^{\Trans}x + b)$, where $w$ is drawn from the probability distribution $p$:
\begin{equation}
p(w) = \frac{1}{2\pi} \int k(h) ~ \text{exp}(-iw^{\Trans}h) ~ \text{d}h
\label{eq:probFourier}
\end{equation} 
 
when we compute the Fourier transform of the kernel $k$, and $b$ is drawn from the uniform distribution on $[0, 2\pi]$.

We know that the fourier transform of $k(\cdot)$ is a probability distribution from Bochner's theorem:
\begin{thm} \label{thm:Bochner}
	(Bochner \cite{Rudin_1990}).For a continuous kernel $k(x - y)$  it is a positive definite kernel if and only if $k(\cdot)$ is the fourier transform of a non-negative measure.
\end{thm}
We now have:
\[ k(x - y) = \int_{\RR^d} p(w) \text{exp}(iw^{\Trans}(x - y)) ~ \text{d}w = \EE_w[e^{iw^{\Trans}x}(e^{iw^{\Trans}y})^*] \].
Therefore, we can use $e^{iw^{\Trans}x}(e^{iw^{\Trans}y})^*$ as an estimate (unbiased) of $k(x, y)$.
Let $\phi_w(x) = e^{iw^{\Trans}x}$
We can also use $z_w(x) = \sqrt{2}cos(w^{\Trans}x + b)$ instead of $\phi_w(x)$, as suggested by \cite{RFF_Rahimi}.
\begin{prop}
	For $z_w(x) = \sqrt{2}cos(w^{\Trans}x + b)$, where $w$ is drawn from probability distribution $p$ in \eqref{eq:probFourier} and $b$ drawn from a uniform random variable on $[0, 2\pi]$.
	\[E(z_w(x))z_w(y) = k(x,y)\]
\end{prop}
\begin{proof}
	\begin{equation*}
	\begin{split}
		z_w(x) &= 2 ~ \frac{\sqrt{2}}{2}cos(w^{\Trans}x + b) \\
		&= \frac{1}{\sqrt{2}}~(e^{i(w^{\Trans}x+b)} + e^{-i(w^{\Trans}x+b)}) \\
		&= \frac{1}{\sqrt{2}}~(\phi_w(x)e^{ib} + \phi_w(x)^* e^{-ib})
	\end{split}
	\end{equation*}
	Where $\phi_w(x) = e^{iw^{\Trans}x}$.
	\begin{equation*}
	\begin{split}
	z_w(x)z_y(y) &= \frac{1}{2}[\phi_w(x)\phi_w(y)e^{i2b} + \phi_w(x)^*\phi_w(y)^*e^{-i2b}
	 + \phi_w(x)\phi_w(y)^* + \phi_w(x)^*\phi_w(y)] \\
	\EE[z_w(x)z_y(y)] &= \frac{1}{2}\EE[\phi_w(x)\phi_w(y)e^{i2b} + \phi_w(x)^*\phi_w(y)^*e^{-i2b}]
	+ \frac{1}{2}\EE[\phi_w(x)\phi_w(y)^*] + \frac{1}{2}\EE[\phi_w(x)^*\phi_w(y)] \\ 
	\end{split}
	\end{equation*}
	As mentioned earlier in Theorem \ref{thm:Bochner}, $\EE_w[\phi_w(x)\phi_w(y)^*] = k(x - y)$.
	Also $\phi_w(x)\phi_w(y)^* = (\phi_w(x)^*\phi_w(y))^*$.
	\begin{equation*}
	\begin{split}
	\EE[z_w(x)z_y(y)] &= \frac{1}{2}\EE[\phi_w(x)\phi_w(y)e^{i2b} + \phi_w(x)^*\phi_w(y)^*e^{-i2b}]
	+ \frac{1}{2}k(x-y) + \frac{1}{2}[k(x-y)]^* \\
	&= \frac{1}{2}\EE[\phi_w(x)\phi_w(y)e^{i2b} + \phi_w(x)^*\phi_w(y)^*e^{-i2b}] + k(x-y)
	\end{split}
	\end{equation*}
	For real kernel , $k(x - y) = (k(x - y))^*$.
	\begin{equation*}
	\begin{split}
	\EE_{w,b}[\phi_w(x)\phi_w(y)e^{i2b}] &= \frac{1}{2\pi}\int_{\RR^d}\int_{0}^{2\pi} p(w)\phi_w(x)\phi_w(y)e^{i2b} \text{d}b~\text{d}w \\
	&= \frac{1}{2\pi}\int_{\RR^d} p(w)\phi_w(x)\phi_w(y) \int_{0}^{2\pi} e^{i2b} \text{d}b~\text{d}w \\
	&= 0
	\end{split}
	\end{equation*}
	Since $\int_{0}^{2\pi} e^{i2b} \text{d}b = 0$. Similarly, $	\EE_{w,b}[\phi_w(x)^*\phi_w(y)^*e^{-i2b}] = 0$.
	\[ \therefore \EE[z_w(x)z_y(y)] = k(x - y). \]
	
\end{proof}
As suggested by \cite{RFF_Rahimi}, the variance of the estimate is decreased by using $z$, a $D$ dimensional vector by concatenating $D$ of $z_w$ and normalizing by a constant $\sqrt{D}$. We let:
\[z(x) = \sqrt{\frac{2}{D}} [cos(w_1^{\Trans}x + b_1) ... cos(w_D^{\Trans}x + b_D)] \]
with randomly drawn $w_i$ and $b_i$ as described previously.


\begin{thm} \label{thm:RFFnorm}
	For $N$ the number of random features, and $x_1, x_2, ..., x_n$ the data points, when $N > n$ and as $N$ increases, the norm of the minimizer tends to  the norm of the minimum norm RKHS interpolant.
\end{thm}
\begin{proof}
Let $f(x)$ be the minimum norm RKHS interpolant function for the datapoints. 
\[f(x) = \sum_{i}\alpha_ik(x_i, x) \approx \sum_{i}\alpha_iz(x_i)^\text{T}z(x) 
 = \beta^\text{T}z(x) = \hat{f}(x)
\] (the first equality holds due to Representer Theorem)
Where $\beta = \sum_{i}\alpha_iz(x_i) $.
The norm of the function from the random fourier features approximation is:
\[ \norm{\beta} =   \beta^\text{T} \bar{\beta} = 
(\sum_{i}\alpha_i z^\text{T}(x_i)) (\sum_{i}\bar{\alpha}_i\bar{z}(x_i)) 
= \sum_i \sum_j \alpha_i \bar{\alpha}_j z^\text{T}(x_i)\bar{z}(x_j)  
\approx \sum_i \sum_j \alpha_i \bar{ \alpha_j} k(x_i, x_j)
= \norm{f}
\]
\end{proof}
\newpage
\bibliographystyle{plain}
\bibliography{refs}

\newpage
{\huge \centering \bf Appendix \par}

\appendix

\end{document} 


